(window.webpackJsonp=window.webpackJsonp||[]).push([[14],{428:function(t,a,s){t.exports=s.p+"assets/img/img_10.08cd6778.png"},429:function(t,a,s){t.exports=s.p+"assets/img/img_11.58c89e27.png"},473:function(t,a,s){"use strict";s.r(a);var n=s(2),r=Object(n.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"背景"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#背景"}},[t._v("#")]),t._v(" 背景")]),t._v(" "),a("p",[t._v("为了降本增效，我们要将谷歌云上面的数据和服务迁移至腾讯云，其中包括 30TB 左右的 Iceberg 数据。Iceberg 数据要从 GCS 迁移到腾讯云的 EMR HDFS 上。本文是此次记录迁移过程及分享期间遇到的问题及解决方案。")]),t._v(" "),a("h2",{attrs:{id:"迁移工具"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#迁移工具"}},[t._v("#")]),t._v(" 迁移工具")]),t._v(" "),a("p",[t._v("我们使用腾讯云 EMR 的 Spark 集群完成了数据迁移。")]),t._v(" "),a("ul",[a("li",[t._v("Spark 3.3_2.12")]),t._v(" "),a("li",[t._v("Iceberg 1.1.0")])]),t._v(" "),a("h2",{attrs:{id:"spark-配置"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#spark-配置"}},[t._v("#")]),t._v(" Spark 配置")]),t._v(" "),a("p",[t._v("Spark 需要配置读取 GCS 文件和读 Iceberg 表：")]),t._v(" "),a("ul",[a("li",[t._v("GCS 配置参考： "),a("a",{attrs:{href:"https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/INSTALL.md",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/INSTALL.md"),a("OutboundLink")],1),t._v(" 。这里需注意，GCP 的凭证文件要添置在各个 Spark 节点下，保证所有的 Spark 节点可以读取。")]),t._v(" "),a("li",[t._v("Iceberg 配置：添加 "),a("code",[t._v("iceberg-spark-runtime-3.3_2.12-1.1.0.jar")]),t._v("到 spark/jars 下。可以在同 emr 中开启 Iceberg 组件，然后执行 "),a("code",[t._v("cp /usr/local/service/iceberg/iceberg-spark-runtime-3.3_2.12-1.1.0.jar /usr/local/service/spark/jars/")])])]),t._v(" "),a("p",[t._v("然后需要配置 spark 连接俩 metastore，为了方便可以修改 spark-default.yaml 配置文件，这样子每次 spark 启动时即应用此份配置。")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hadoop"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("AbstractFileSystem"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("gs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("impl  com"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("google"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cloud"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hadoop"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("gcs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("GoogleHadoopFS\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hadoop"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("google"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cloud"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("auth"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("service"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("account"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("enable  true\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hadoop"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("google"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cloud"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("auth"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("service"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("account"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("json"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keyfile  "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("usr"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("local"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("service"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("hadoop"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("etc"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("hadoop"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("google_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("json\n\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("catalog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hive_prod  org"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iceberg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("SparkCatalog\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("jars"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("packages  org"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iceberg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("iceberg"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("runtime"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.2_2")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v(".12")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.1")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v(".0")]),t._v("\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extensions  org"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iceberg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extensions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("IcebergSparkSessionExtensions\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("catalog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("spark_catalog  org"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iceberg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("SparkSessionCatalog\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("catalog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("spark_catalog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),t._v("  hive\n\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("catalog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tx_iceberg  org"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iceberg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("SparkCatalog\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("catalog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tx_iceberg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),t._v("  hive\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("catalog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tx_iceberg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uri  thrift"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("ip"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("port\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("catalog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("gcp_iceberg  org"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iceberg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("SparkCatalog\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("catalog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("gcp_iceberg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),t._v("  hive\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("catalog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("gcp_iceberg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uri  thrift"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("ip"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("port\n\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("executor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("instances "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("executor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cores "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("executor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("memory "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" 120g\n")])])]),a("p",[t._v("验证配置是否生效，可以在 spark 节点上执行 spark-sql，打开 spark sql 的客户端，简单执行查询下俩边的 iceberg 表和进行插入操作，判断是否读写都有权限。")]),t._v(" "),a("h2",{attrs:{id:"迁移问题及解决方案"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#迁移问题及解决方案"}},[t._v("#")]),t._v(" 迁移问题及解决方案")]),t._v(" "),a("p",[t._v("我们迁移的 Iceberg 表大部分都是按月分区或按日分区，然后有配置 sort order 键排序写入的。")]),t._v(" "),a("p",[t._v("同步的 SQL 语句也非常简单。")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("insert into tx_iceberg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("prod_bronze"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sui_transaction_blocks\nselect "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" gcp_iceberg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("prod_bronze"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sui_transaction_blocks\n")])])]),a("h3",{attrs:{id:"spark-资源不足"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#spark-资源不足"}},[t._v("#")]),t._v(" Spark 资源不足")]),t._v(" "),a("p",[t._v("在刚开始同步时，发现同步速度缓慢。同步一张 10GB 表需要耗时20分钟，太慢了。于是便分析同步的 Spark Job，同步任务的 Spark Stage :")]),t._v(" "),a("ul",[a("li",[t._v("加载数据源")]),t._v(" "),a("li",[t._v("将数据分区")]),t._v(" "),a("li",[t._v("将数据排序写入")])]),t._v(" "),a("p",[a("img",{attrs:{src:s(428),alt:"Untitled"}})]),t._v(" "),a("p",[t._v("因为需要对数据进行分区处理和排序处理，是很吃内存的，我们通过查看 Executor 指标也验证了这点，太多数据 Spill Disk ，这便是导致同步数据慢的原因。我们需要更大的内存来进行同步。")]),t._v(" "),a("p",[a("img",{attrs:{src:s(429),alt:"Untitled"}})]),t._v(" "),a("p",[t._v("于是我们对 Spark 集群进行了扩容处理，考虑到我们的大表都是 700GB 往上，最大的 3TB，于是便将集群资源扩容到 10台"),a("em",[t._v("32c")]),t._v("128g 。")]),t._v(" "),a("p",[t._v("在创建同步任务之前，我们需要指定 Spark Application 使用的资源：")]),t._v(" "),a("p",[a("code",[t._v("spark-sql --master yarn --num-executors 10 --executor-cores 30 --executor-memory 120g")])]),t._v(" "),a("h3",{attrs:{id:"减少-shuffle-和-sort"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#减少-shuffle-和-sort"}},[t._v("#")]),t._v(" 减少 Shuffle 和 Sort")]),t._v(" "),a("p",[t._v("在集群对扩容后，同步速度有了大幅提升；但在同步一张 700GB 的表时，同步速度又慢了下来，慢的主要原因是 Shuffle 太多了，在任务中需要将 1400GB 的数据量（iceberg 数据解压后的数据）进行分区排序，然后 shuffle 到不同 executor 执行 sort order 写入。")]),t._v(" "),a("p",[t._v("我们做了俩项优化来解决了以上的问题：")]),t._v(" "),a("ul",[a("li",[t._v("改变同步模式: 将全量同步改为按分区同步。Spark 默认是将整张表的数据 load 到集群内，并不是 pipeline 方式插入数据的。为了减少任务的压力，我们每次仅传入一个分区的数据，这样子集群压力能减轻且速度更快。")]),t._v(" "),a("li",[t._v("设置 spark-iceberg 配置 "),a("code",[t._v("write.distribution-mode=hash")]),t._v("减少分区排序。在 Iceberg 1.2.0 之前默认是 "),a("code",[t._v("write.distribution-mode=none")]),t._v("，1.2.0 之后是 "),a("code",[t._v("write.distribution-mode=hash")]),t._v("。\n"),a("ul",[a("li",[a("code",[t._v("none")]),t._v("：spark 需要根据分区值对数据进行排序。数据必须在每个 Spark 任务内或在整个数据集中全局排序。")]),t._v(" "),a("li",[a("code",[t._v("hash")]),t._v("：spark 会对每行数据的分区值进行哈希处理，根据哈希结果分配到不同的 executor 中。")])])])]),t._v(" "),a("h3",{attrs:{id:"iceberg-小文件太多"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#iceberg-小文件太多"}},[t._v("#")]),t._v(" Iceberg 小文件太多")]),t._v(" "),a("p",[t._v("我们在同步一张 4G 的表时，发现同步速度很慢，同步一张 4G 的表需要耗时30分钟。打开 Spark Job 分析任务的 Stage，发现读取 Iceberg 数据源的 Task 有 2000 个。每个 Task 做的事情都是从读取 GCP iceberg 的一个数据文件 load 回 Spark 中。\n然后我们查看了这张 Iceberg 表的文件数量和文件大小，发现全是小文件。")]),t._v(" "),a("p",[t._v("为了解决小文件的问题，我们可以通过 Trino 或 Spark 进行小文件合并，注意要在同一个云服务内进行合并，不要跨云进行文件合并。\nSpark 合并小文件 SQL 参考：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CALL")]),t._v(" catalog_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("system"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rewrite_data_files"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'db.sample'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h3",{attrs:{id:"瓶颈分析"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#瓶颈分析"}},[t._v("#")]),t._v(" 瓶颈分析")]),t._v(" "),a("p",[t._v("此次同步过程中，依赖到 GCS、HDFS、Spark 三个集群，当同步速度慢时，我们需要分析集群内的短板，网络带宽，磁盘速率等因素。")]),t._v(" "),a("p",[t._v("以下是此次同步的做了资源升配的操作：")]),t._v(" "),a("ul",[a("li",[t._v("spark 集群 worker 节点升配: 3 * 8c"),a("em",[t._v("16g → 10 * 32c")]),t._v("128g，主要吃内存其次是 cpu")]),t._v(" "),a("li",[t._v("hdfs 集群 worker 节点升配: 3 * 8c"),a("em",[t._v("16g → 3 * 32c")]),t._v("128g ，主要吃 cpu 其次是内存")])]),t._v(" "),a("h2",{attrs:{id:"总结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[t._v("#")]),t._v(" 总结")]),t._v(" "),a("ul",[a("li",[t._v("大表需要考虑按分区同步，让同步更轻更快。")]),t._v(" "),a("li",[t._v("iceberg 表配置 "),a("code",[t._v("write.distribution-mode=hash")]),t._v("，避免 spark partition sort 操作，根据 hash 更快完成分区。")]),t._v(" "),a("li",[t._v("需要留意同步的 Iceberg 小文件数量是否异常，如果异常需要进行合并")]),t._v(" "),a("li",[t._v("同步慢时需分析 spark job 和集群资源消耗状况。")])])])}),[],!1,null,null,null);a.default=r.exports}}]);