(window.webpackJsonp=window.webpackJsonp||[]).push([[14],{421:function(t,s,a){t.exports=a.p+"assets/img/img_6.f5659649.png"},449:function(t,s,a){"use strict";a.r(s);var r=a(2),e=Object(r.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h2",{attrs:{id:"数据治理"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#数据治理"}},[t._v("#")]),t._v(" 数据治理")]),t._v(" "),s("h3",{attrs:{id:"flink-实时写入-iceberg-带来的问题"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#flink-实时写入-iceberg-带来的问题"}},[t._v("#")]),t._v(" Flink 实时写入 Iceberg 带来的问题")]),t._v(" "),s("p",[t._v("在实时数据源源不断经过 Flink 写入的 Iceberg 的过程中，Flink 通过定时的 Checkpoint 提交 snapshot commit 操作到 Iceberg，将已写入到 Iceberg 的数据文件通过 Snapshot 组织暴露出来。如果不对流实时写入 Iceberg 的文件进行治理，久而久之 Iceberg 下的小文件会越来越多，Snapshot 版本也越来越多，查询速度大打折扣。")]),t._v(" "),s("h3",{attrs:{id:"数据治理方案"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#数据治理方案"}},[t._v("#")]),t._v(" 数据治理方案")]),t._v(" "),s("p",[t._v("基于上述问题，我们需要对 Iceberg 的元数据和数据文件定期进行治理。治理方向主要有俩点：")]),t._v(" "),s("ul",[s("li",[t._v("清理快照")]),t._v(" "),s("li",[t._v("合并小文件")])]),t._v(" "),s("p",[t._v("因为我们查询引擎用 Trino，于是我们选用 Trino 对 Iceberg 进行优化。\nTrino-Iceberg Connetor 提供了优化方法：")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 清理快照")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ALTER")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" test_table "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("EXECUTE")]),t._v(" remove_orphan_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("retention_threshold "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'7d'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 合并小文件")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ALTER")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" test_table "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("EXECUTE")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("optimize")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("file_size_threshold "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'10MB'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("使用 Trino SQL 便可以对 Iceberg 表进行优化，很方便。我们基于 Trino SQL 上，做了一个自动自助的 Iceberg 表优化工具，实现了定时对某个 Catalog 下的表进行优化，省去了人工运维优化的成本。\n除了快照清理和合并小文件外，Trino 提供了清理无效数据的方法，可以删掉一些已经不被 Iceberg 管理的无用的数据文件。我们是每周对 Iceberg 执行一次无效数据清理。")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 清理无效文件")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ALTER")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" test_table "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("EXECUTE")]),t._v(" remove_orphan_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("retention_threshold "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'7d'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"查询加速"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#查询加速"}},[t._v("#")]),t._v(" 查询加速")]),t._v(" "),s("p",[t._v("我们都知道对 Iceberg Partition 列进行查询速度都很快，因为其过滤掉很多文件，只读取符合查询分区的数据文件。单读到底层的 ORC 数据文件时，Iceberg 提供了 min/max 等数据元信息，通过元信息可以快速得知所找的数据是否在此文件内。")]),t._v(" "),s("h3",{attrs:{id:"bloom-filter"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#bloom-filter"}},[t._v("#")]),t._v(" Bloom Filter")]),t._v(" "),s("p",[t._v("在最新的 Iceberg 1.1.0 版本中，Iceberg 支持在 ORC 数据文件内设置 bloom filters。\n而新版 Trino 也跟上 Iceberg 适配 bloom filter，我们需要在 trino-iceberg 的配置文件里配置，来开启 Trino 查询时使用 bloom filter 查询")]),t._v(" "),s("div",{staticClass:"language-yaml extra-class"},[s("pre",{pre:!0,attrs:{class:"language-yaml"}},[s("code",[t._v("hive.orc.bloom"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("filters.enabled = true\n")])])]),s("p",[t._v("除此之外，我们还需要设置 Iceberg 表属性，对列配置上 bloom filter")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CREATE")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" iceberg_table "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n   token_address "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("varchar")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n   from_address "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("varchar")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n   to_address "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("varchar")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n   block_timestamp "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("timestamp")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("time")]),t._v(" zone"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WITH")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n   orc_bloom_filter_columns "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ARRAY"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'token_address'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'from_address'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'to_address'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n   orc_bloom_filter_fpp "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.05")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n   partitioning "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ARRAY"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'day(block_timestamp)'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("因为 bloom filter 是生效于 ORC 文件中，如果想要应用在旧表上，需要将旧表数据重写到新表上，这样底层的数据文件才带有 bloom filter。")]),t._v(" "),s("p",[t._v("实战应用及效果：\n假如我们有一张 token_transfer 表，表内大概有四个字段")]),t._v(" "),s("ul",[s("li",[t._v("from_address 买方地址")]),t._v(" "),s("li",[t._v("to_address 卖家地址")]),t._v(" "),s("li",[t._v("token_address 交易代币")]),t._v(" "),s("li",[t._v("block_timestamp 日期")])]),t._v(" "),s("p",[t._v("我们对该表 from_address、to_address、token_address 应用 bloom filter，对 timestamp 进行分区。该表每天的数据量假设有 100w 条数据。\n此时有俩类查询过来：")]),t._v(" "),s("ul",[s("li",[t._v("查询热门 token 今天发生的交易")])]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" token_transfer \n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" token_address "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'热门token'")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("and")]),t._v(" block_timestamp "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" today\n")])])]),s("ul",[s("li",[t._v("查询冷门 token 今天发生的交易")])]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" token_transfer \n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" token_address "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'冷门token'")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("and")]),t._v(" block_timestamp "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" today\n")])])]),s("p",[t._v("此时俩类查询的 bloom filter 产生的效果是不一样的，因为热门的 token 会存在大部分数据文件里，冷门的 token 大概率只存在于少部分数据文件内。对于热门 token，bloom filter 的加速效果不佳，但对于冷门 token，bloom filter 帮助其快速过滤掉了很多数据文件，快速找到有冷门 token 的数据文件，加速效果极佳。\n所以得到的结论是，bloom filter 对一些 不重复，特征值很高的数据有比较好的加速效果。")]),t._v(" "),s("h3",{attrs:{id:"order-z-order"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#order-z-order"}},[t._v("#")]),t._v(" Order & Z-Order")]),t._v(" "),s("p",[t._v("上文提到，ORC数据文件内有 min/max 值，查询引擎可以根据 min/max 值判断数据是否在此文件内。\n可是日常在写入 Iceberg 的数据一般都是无序写入的，无序写入会导致每个数据文件也是无序的，不能发挥 min/max 过滤的效果。")]),t._v(" "),s("h4",{attrs:{id:"order"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#order"}},[t._v("#")]),t._v(" Order")]),t._v(" "),s("p",[t._v("Spark 提供了一个压缩文件并排序的方法，可以将无序的文件按指定列排好序。排序策略不仅可以优化文件大小，还可以对数据进行排序以对数据进行聚类以获得更好的性能。将相似数据聚集在一起的好处是更少的文件可能具有与查询相关的数据，这意味着 min/max 的好处会更大（扫描的文件越少，速度越快）。")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CALL")]),t._v(" catalog"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("system"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rewrite_data_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'db.teams'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n  strategy "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sort'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n  sort_order "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'team ASC NULLS LAST, name DESC NULLS FIRST'")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:a(421),alt:"image.png"}})]),t._v(" "),s("h4",{attrs:{id:"z-order"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#z-order"}},[t._v("#")]),t._v(" Z-Order")]),t._v(" "),s("p",[t._v("虽然 Order 排序可以同时对多列进行排序，但其列与列之间的排序是有先后顺序之分的，像是 MySQL 里的联合索引，先对 字段A 排序再对 字段B 排序。如果只是的查询的谓词只包含 字段B，则上述索引失效（先对 字段A 排序再对 字段B 排序)。\n而 Z-Order 能解决上面的问题，使用 Z-Order 对多列排序，列与列之间的排序权重相同。所以使用 Z-Order 对多字段进行排序，查询中只要谓词命中了 Z-Order 中其中任何一字段，都能加速查询。\nSpark 提供了使用 Z-Order 的方法")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CALL")]),t._v(" catalog"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("system"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rewrite_data_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'db.people'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n  strategy "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sort'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n  sort_order "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'zorder(height_in_cm, age)'")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h4",{attrs:{id:"差异"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#差异"}},[t._v("#")]),t._v(" 差异")]),t._v(" "),s("p",[t._v("我们测试过对 100G 的表分别进行 Order 和 Z-Order，命中 Order 最高能带来 10 倍的性能提升，命中 Z-Order 能带来 2 倍的性能提升。粗步得到的结论是，Order 比 Z-Order 大致快 2 倍。\n所以在实践应用上不能盲目选择 Z-Order，得根据这张表的热门查询SQL、字段特征、数量来做：")]),t._v(" "),s("ul",[s("li",[t._v("查询字段是数据连续且范围小的，选 Order")]),t._v(" "),s("li",[t._v("查询字段具有高基数特征，选 Z-Order")]),t._v(" "),s("li",[t._v("频繁查询此表多个字段的，选 Z-Order，否则 Order 的性能会更好")])]),t._v(" "),s("h2",{attrs:{id:"小结"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#小结"}},[t._v("#")]),t._v(" 小结")]),t._v(" "),s("p",[t._v("Iceberg 做了很多功夫去加速查询，本文中提到的小文件合并、快照清理、Bloom Filter、Order、Z-Order 都是为了在查询时跳过无用的文件，通过减少磁盘 IO 操作来加速查询。Trino 和 Spark 提供许多便利的方法给开发者维护治理 Iceberg；数据治理这块成本比较低，可以写好自动化脚本每天执行数据治理；查询加速这里的维护成本比较高，都是需要重写元数据和数据文件的操作，一般每月做一次重写操作。")]),t._v(" "),s("h2",{attrs:{id:"参考文章"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#参考文章"}},[t._v("#")]),t._v(" 参考文章：")]),t._v(" "),s("ul",[s("li",[t._v("https://www.dremio.com/blog/compaction-in-apache-iceberg-fine-tuning-your-iceberg-tables-data-files/")]),t._v(" "),s("li",[t._v("https://www.dremio.com/blog/how-z-ordering-in-apache-iceberg-helps-improve-performance/")])])])}),[],!1,null,null,null);s.default=e.exports}}]);