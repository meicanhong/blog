---
title: How to build a more fast and stable crawler
date: '2024-12-21'
categories:
- Program
tags:
- Crawler
---
# How to build a more fast and stable crawler
## #1 ä¸è¦é‡å¤é€ è½®å­
RSSHub æ˜¯ä¸€ä¸ªå¼€æºçš„ RSS è®¢é˜…æœåŠ¡ï¼Œæœ‰å¾ˆå¤šå¼€å‘è€…åœ¨ä¸Šé¢å¼€å‘äº†å„ç§å„æ ·çš„è®¢é˜…æºï¼Œæ¯”å¦‚ï¼š
- twitter
- youtube
- çŸ¥ä¹
- è±†ç“£
- ç­‰ç­‰

è¿™äº›éƒ½æ˜¯é«˜è´¨é‡çš„æ•°æ®æºï¼Œå¦‚æœè‡ªå·±ä»å¤´å¼€å§‹å†™ï¼Œä¸ä»…æµªè´¹æ—¶é—´ï¼Œè€Œä¸”è´¨é‡è¿˜å¯èƒ½ä¸å¦‚è¿™äº›ç°æˆçš„ã€‚æˆ‘ä»¬åªéœ€è¦å†™ç¨‹åºå»æ¥æ”¶ç»Ÿä¸€çš„ RSS è®¢é˜…æºè¿”å›çš„æ•°æ®ï¼Œç„¶åè¿›è¡Œå¤„ç†ï¼Œå°±å¯ä»¥å¿«é€Ÿè·å–å„ä¸ªå¹³å°çš„æ•°æ®ã€‚

æ‹¿Twitterä¸¾ä¾‹ï¼Œæˆ‘ä»¬åªéœ€è¦æä¾› Twitter Cookie é…ç½®åˆ° RSSHub ä¸­ï¼Œè®¿é—® RSSHub çš„ API `http://localhost:1200/twitter/user/yihui_indie` å°±å¯ä»¥è·å–åˆ° Twitter ç”¨æˆ· yihui_indie çš„æ¨æ–‡ã€‚

``` yaml
<item>
<title>Re @boo_hz å“ˆå“ˆå“ˆï¼Œå…¶å®æ˜¯æœ‰çš„ã€‚ä½†ç®¡ä»–å‘¢ğŸ˜¬</title>
<description>Re @boo_hz å“ˆå“ˆå“ˆï¼Œå…¶å®æ˜¯æœ‰çš„ã€‚ä½†ç®¡ä»–å‘¢ğŸ˜¬</description>
<link>https://x.com/yihui_indie/status/1870409487733149905</link>
<guid isPermaLink="false">https://twitter.com/yihui_indie/status/1870409487733149905</guid>
<pubDate>Sat, 21 Dec 2024 10:02:27 GMT</pubDate>
<author>ç† è¾‰ Indie</author>
</item>

<item>
<title>Re @hixiaoji ä¼°è®¡ä¹Ÿæ˜¯ä½ æ”¶åˆ°äº§å“å»ºè®®æœ€å¤šçš„ä¸€æ¡åŠ¨æ€å§ğŸ¤£</title>
<description>Re @hixiaoji ä¼°è®¡ä¹Ÿæ˜¯ä½ æ”¶åˆ°äº§å“å»ºè®®æœ€å¤šçš„ä¸€æ¡åŠ¨æ€å§ğŸ¤£</description>
<link>https://x.com/yihui_indie/status/1870335626970866059</link>
<guid isPermaLink="false">https://twitter.com/yihui_indie/status/1870335626970866059</guid>
<pubDate>Sat, 21 Dec 2024 05:08:57 GMT</pubDate>
<author>ç† è¾‰ Indie</author>
</item>
```
## #2 å€ŸåŠ©LLMå®Œæˆæ•°æ®å¤„ç†
æˆ‘ä»¬è·å–ä¸€ä¸ªå¹³å°çš„æ•°æ®å†…å®¹ï¼Œé€šå¸¸éœ€è¦å¯¹é½å¹³å°è¿›è¡Œç‰¹åˆ¶åŒ–å¤„ç†ï¼Œæ¯”å¦‚ï¼š
- æ‹›è˜å¹³å°éœ€è¦æå–å‡ºèŒä½åç§°ã€èŒä½é“¾æ¥ã€èŒä½å‘å¸ƒæ—¶é—´ã€èŒä½å…¬å¸ã€èŒä½åœ°ç‚¹ã€èŒä½è–ªèµ„ã€èŒä½è¦æ±‚ç­‰
- çŸ¥ä¹éœ€è¦æå–å‡ºå›ç­”å†…å®¹ã€å›ç­”é“¾æ¥ã€å›ç­”å‘å¸ƒæ—¶é—´ã€å›ç­”ä½œè€…ã€å›ç­”ç‚¹èµæ•°ã€å›ç­”è¯„è®ºæ•°ç­‰
- ç­‰ç­‰

ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ browserless å¾ˆè½»æ¾å¯ä»¥è·å–ç½‘é¡µçš„æºæ•°æ®(åŒ…è£¹HTMLçš„æ•°æ®å†…å®¹)ï¼Œç„¶åé€šè¿‡ bs4 å»é™¤é‡Œé¢æ— ç”¨çš„æ ‡ç­¾ï¼Œå¯ä»¥è·å¾—ä¸€ä»½å¹²å‡€ç®€çŸ­çš„HTMLæ•°æ®ã€‚è¿™æ—¶å€™æˆ‘ä»¬å¯ä»¥å°†è¿™ä»½æ•°æ®äº¤ç»™LLMï¼Œè®©LLMä»ä¸­æå–å‡ºæˆ‘ä»¬é¢„æœŸçš„ struct æ•°æ®ã€‚

ä¸¾ä¾‹ï¼šæˆ‘æƒ³è¦è·å– https://gmgn.ai/trade?chain=sol&tab=smart_degen çš„èªæ˜é’±æ•°æ®ï¼Œæˆ‘ä¼šåˆ©ç”¨ firecrawl çš„ crawl api å®Œæˆè¿™ä¸ªä»»åŠ¡

``` python
class GmgnTokenInfo(BaseModel):
    name: str
    volume: str
    holders: str
    volume_1h: str
    price: str


class GmgnTokenInfoList(BaseModel):
    tokens: list[GmgnTokenInfo]


def crawl_gmgn():
    app = firecrawl_app()
    data = app.scrape_url("https://gmgn.ai/?chain=sol", {
        'formats': ['extract'],
        'extract': {
            'schema': GmgnTokenInfoList.model_json_schema()
        }
    })
    print(json.dumps(data, indent=4))
```
è¾“å‡º
``` json
{
        "tokens": [
            {
                "name": "GOBLY",
                "volume": "$6.3K",
                "holders": "28,202",
                "volume_1h": "7.3K",
                "price": "$0.0\u20856389"
            },
            {
                "name": "USACOIN",
                "volume": "$11.4M",
                "holders": "21,530",
                "volume_1h": "472.1K",
                "price": "$0.0114"
            }
            ...
        ]
    }
```

è¿™é‡Œæˆ‘ç”¨åˆ°äº† firecrawl å·¥å…·ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¼ºå¤§çš„çˆ¬è™«å·¥å…·ï¼Œå®ƒå¯ä»¥è·å–ä»»æ„ç½‘é¡µçš„æ•°æ®å†…å®¹ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨å®ƒçš„ extract åŠŸèƒ½ï¼Œå°† `GmgnTokenInfoList` çš„ç»“æ„ä¸¢ç»™å®ƒï¼Œfirecrwal èƒŒåå…¶å®æ˜¯è°ƒç”¨ LLM å®Œæˆæ•°æ®æå–ã€‚

firecrawl scrape è¯¦æƒ…å¯ä»¥å‚è€ƒï¼šhttps://docs.firecrawl.dev/v1-welcome#extract-format

### 2024-12-28 æ›´æ–°
firecrawl ç›®å‰ extract æ¨¡å¼ä¸å¤ªç¨³å®šï¼Œæ‰€ä»¥è¿˜æ˜¯æ¨èè‡ªå·±å†™ agent å®Œæˆæ•°æ®æå–ã€‚
ä»¥ä¸‹æ˜¯æˆ‘å†™çš„ä¸€ä¸ªä» content ä¸­æå–å‡º struct çš„ agent ç¤ºä¾‹ï¼š

```python
import json
from typing import Any, Dict

import structlog

from src.util.openai_util import query_llm

logger = structlog.get_logger()


async def extract_structured_data(
    content: str, schema: Dict[str, Any]
) -> Dict[str, Any]:
    if not content.strip() or not schema:
        raise ValueError("Content and schema cannot be empty")

    # Construct the extraction prompt
    prompt = f"""
Please extract information from the following text and format it according to the schema.
Respond ONLY with a valid JSON object matching the schema structure.

Text content:
{content}

Required schema:
{json.dumps(schema, indent=2)}

Rules:
1. Extract all relevant information that matches the schema
2. Use null for missing values
3. Strictly follow the schema structure
4. Return only the JSON object, no additional text
"""

    try:
        # Query LLM with JSON response format
        response = await query_llm(query=prompt, response_json=True)

        # Parse and validate the response
        extracted_data = json.loads(response)

        return extracted_data

    except json.JSONDecodeError:
        raise Exception("Failed to parse LLM response as JSON")
    except Exception as e:
        raise Exception(f"Data extraction failed: {str(e)}")


if __name__ == "__main__":
    import asyncio

    # ç¤ºä¾‹æ–‡æœ¬å’Œæ¨¡å¼
    text_content = """
    John Doe is a software engineer at Tech Corp.
    He has 5 years of experience and specializes in Python and JavaScript.
    Contact: john.doe@email.com
    """

    schema = {
        "name": "string",
        "occupation": "string",
        "company": "string",
        "years_of_experience": "number",
        "skills": ["string"],
        "contact": {"email": "string"},
    }

    # æå–æ•°æ®
    result = asyncio.run(extract_structured_data(text_content, schema))
    print(result)

```